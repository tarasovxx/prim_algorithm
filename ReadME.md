# **Параллельный алгоритм Прима (MPI/OMP/гибридный подход)**

## **Как работает алгоритм Прима?**

Алгоритм Прима относится к классу жадных алгоритмов, которые находят локально оптимальное решение в надежде, что оно приведет к глобально оптимальному. Мы начинаем с одной вершины и последовательно добавляем рёбра с минимальным весом, пока не получим минимальное остовное дерево (MST).

**Шаги работы алгоритма Прима:**

1. Инициализируем минимальное остовное дерево, выбирая случайную вершину.
2. Находим все рёбра, соединяющие текущее дерево с новыми вершинами, выбираем минимальное и добавляем его в MST.
3. Повторяем шаг 2, пока не получим минимальное остовное дерево.

Алгоритм работает за время **O(n²)** в стандартной реализации.

---

## **Распараллеливание алгоритма Прима с помощью MPI**

Параллельную реализацию можно организовать следующим образом:

1. Разбиваем множество вершин **V** на **p** подмножеств так, чтобы каждое подмножество содержало **n/p** последовательных вершин и их рёбра. Каждому процессу MPI назначается одно из подмножеств. Каждый процесс также содержит часть массива **d**, которая отвечает за его подмножество вершин. В таком разбиении смежности на процессы иллюстрируется на рисунке 1.
2. Каждый процесс **p_i** находит **локально** минимальное ребро **e_i**, которое соединяет MST с вершиной из его подмножества **V_i**.
3. Каждый процесс **p_i** отправляет своё минимальное ребро **e_i** корневому процессу с помощью **all-to-one reduction** (сбор данных от всех процессов).
4. Корневой процесс выбирает глобально минимальное ребро **e_min** среди всех присланных рёбер, добавляет его в MST и **рассылает** (broadcast) всем остальным процессам.
5. Все процессы отмечают вершины, соединённые **e_min**, как принадлежащие MST и обновляют соответствующую часть массива **d**.
6. Повторяем шаги 2–5, пока все вершины не будут включены в MST.

---

## **Время работы и сложность параллельного алгоритма**

- Поиск минимального ребра и обновление массива **d** в каждой итерации требует времени **O(n/p)**.
- Каждая итерация также включает два вида коммуникационных затрат:
    - **MPI_Reduce (сбор минимумов)**
    - **MPI_Bcast (рассылка информации о новом рёбре)**
- Эти коллективные операции завершаются за **O(log p)**.

**Общая сложность параллельного алгоритма:**  
\[
T_p = O(n^2 / p) + O(n \log p)
\]
где **p** — число процессов MPI.

---

## **Анализ эффективности**

При сравнении времени коммуникации и вычислений можно заметить, что алгоритм Прима **тратит большую часть времени на операции связи** между процессами.

- При увеличении числа процессов **большая часть времени работы алгоритма уходит на коммуникации**.
- Основным **узким местом (bottleneck)** являются коллективные операции **MPI_Reduce** и **MPI_Bcast**, которые требуют синхронизации всех процессов.
- Эти операции **дороже**, чем локальные вычисления внутри каждого процесса, так как каждый процесс **ждет** завершения операции передачи данных по сети.

Из-за этих особенностей алгоритм Прима не масштабируется идеально с увеличением числа процессов. Наилучшая эффективность достигается **при минимально возможном числе процессов**, достаточном для хранения и обработки разбиения входного графа.

---

## **Как запустить проект?**

### **Запуск кода MPI**

1. Открыть терминал в Ubuntu.
2. Перейти в директорию проекта.
3. Скомпилировать код с помощью команды:
   ```bash
   mpicc prims.c -o prims.out
   ```  
4. Запустить код на **4 процессах MPI**:
   ```bash
   mpiexec -np 4 ./prims.out
   ```  

### **Запуск кода с OpenMP (без MPI, только многопоточно)**

```bash
gcc -fopenmp prims.c -o prims.out
./prims.out
```

### **Запуск гибридного кода (MPI + OpenMP)**

```bash
gcc -fopenmp prims.c -o prims.out
mpiexec -np 4 ./prims.out
```

---

## **Дополнительные детали**

- **`prims.c`** — файл исходного кода.
- **`prims.out`** — исполняемый файл (формат старых Unix-подобных ОС для бинарников).
- **4** — количество процессов в MPI.

---
