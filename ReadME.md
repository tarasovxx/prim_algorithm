# **Параллельный алгоритм Прима (OpenMP + MPI). Mephi 2024-25.**

## **Как работает алгоритм Прима?**

Алгоритм Прима относится к классу жадных алгоритмов, которые находят локально оптимальное решение в надежде, что оно приведет к глобально оптимальному. Мы начинаем с одной вершины и последовательно добавляем рёбра с минимальным весом, пока не получим минимальное остовное дерево (MST).

**Шаги работы алгоритма Прима:**

1. Инициализируем минимальное остовное дерево, выбирая случайную вершину.
2. Находим все рёбра, соединяющие текущее дерево с новыми вершинами, выбираем минимальное и добавляем его в MST.
3. Повторяем шаг 2, пока не получим минимальное остовное дерево.

Алгоритм работает за время **O(n²)** в стандартной реализации.

---

## **Распараллеливание алгоритма Прима с помощью MPI**

Параллельную реализацию можно организовать следующим образом:

1. Разбиваем множество вершин **V** на **p** подмножеств так, чтобы каждое подмножество содержало **n/p** последовательных вершин и их рёбра. Каждому процессу MPI назначается одно из подмножеств. Каждый процесс также содержит часть массива **d**, которая отвечает за его подмножество вершин. В таком разбиении смежности на процессы иллюстрируется на рисунке 1.
2. Каждый процесс **p_i** находит **локально** минимальное ребро **e_i**, которое соединяет MST с вершиной из его подмножества **V_i**.
3. Каждый процесс **p_i** отправляет своё минимальное ребро **e_i** корневому процессу с помощью **all-to-one reduction** (сбор данных от всех процессов).
4. Корневой процесс выбирает глобально минимальное ребро **e_min** среди всех присланных рёбер, добавляет его в MST и **рассылает** (broadcast) всем остальным процессам.
5. Все процессы отмечают вершины, соединённые **e_min**, как принадлежащие MST и обновляют соответствующую часть массива **d**.
6. Повторяем шаги 2–5, пока все вершины не будут включены в MST.

---


## **Анализ эффективности**

При сравнении времени коммуникации и вычислений можно заметить, что алгоритм Прима **тратит большую часть времени на операции связи** между процессами.

- При увеличении числа процессов **большая часть времени работы алгоритма уходит на коммуникации**.
- Основным **узким местом (bottleneck)** являются коллективные операции **MPI_Reduce** и **MPI_Bcast**, которые требуют синхронизации всех процессов.
- Эти операции **дороже**, чем локальные вычисления внутри каждого процесса, так как каждый процесс **ждет** завершения операции передачи данных по сети.

Из-за этих особенностей алгоритм Прима не масштабируется идеально с увеличением числа процессов. Наилучшая эффективность достигается **при минимально возможном числе процессов**, достаточном для хранения и обработки разбиения входного графа.

---
